

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ProtMPNN 阅读笔记 (临时) &mdash; My Notes 1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=92fd9be5" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js?v=8930e309"></script>
      <script src="_static/doctools.js?v=888ff710"></script>
      <script src="_static/sphinx_highlight.js?v=4825356b"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="人工智能方面的术语" href="glossary_ai.html" />
    <link rel="prev" title="git 用法教程" href="daily/git_usage.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            My Notes
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="daily/202412.html">2024 年 12 月 - 日知录</a></li>
<li class="toctree-l1"><a class="reference internal" href="daily/git_usage.html">git 用法教程</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">ProtMPNN 阅读笔记 (临时)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id1">摘要</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id2">相关工作</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">动机与尝试</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id6">ProtMPNN 设计的思路.</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id7">训练</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#single-chain-models">训练 Single chain models</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multi-chain-models">训练 multi chain models.</a></li>
<li class="toctree-l4"><a class="reference internal" href="#loss-functions-optimization">Loss functions &amp; optimization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#input-features">Input features</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-arch">模型架构</a></li>
<li class="toctree-l3"><a class="reference internal" href="#in-sillico-analysis">更多虚拟验证 (in sillico analysis)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id12">论文结论</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id14">我的问题</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="glossary_ai.html">人工智能方面的术语</a></li>
<li class="toctree-l1"><a class="reference internal" href="anaconda-tutorial.html">Anaconda 安装以及使用教程</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">My Notes</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">ProtMPNN 阅读笔记 (临时)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/protmpnn.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="protmpnn">
<h1>ProtMPNN 阅读笔记 (临时)<a class="headerlink" href="#protmpnn" title="Permalink to this heading"></a></h1>
<p>MPNN 是 Message-Passing Neural Network 的缩写.</p>
<p>本文目的: 目标: 设计一个可以应用在单体 &amp; 环状低聚体 &amp; protein nanoparticles &amp;
PPI (protein-proten interface) 的 DeepLearning-based sequence desing method.</p>
<section id="id1">
<h2>摘要<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h2>
<p>Although deep learning has revolutionized protein structure prediction, almost
all experimentally characterized de novo protein designs have been generated
using physically based approaches such as Rosetta. Here, we describe a deep
learning–based protein sequence design method, ProteinMPNN, that has
outstanding performance in both in silico and experimental tests. On native
protein backbones, ProteinMPNN has a sequence recovery of 52.4% compared with
32.9% for Rosetta. The amino acid sequence at different positions can be
coupled between single or multiple chains, enabling application to a wide range
of current protein design challenges. We demonstrate the broad utility and high
accuracy of ProteinMPNN using x-ray crystallography, cryo electron microscopy,
and functional studies by rescuing previously failed designs, which were made
using Rosetta or AlphaFold, of protein monomers, cyclic homo-oligomers,
tetrahedral nanoparticles, and target-binding proteins.</p>
</section>
<section id="id2">
<h2>相关工作<a class="headerlink" href="#id2" title="Permalink to this heading"></a></h2>
<dl>
<dt>基于物理的方法:</dt><dd><p>Rosetta - 将序列设计看成一个能量优化问题.</p>
</dd>
<dt>深度学习方法:</dt><dd><p>优点: 给定单体蛋白骨架, 可迅速生成序列, 无需考虑侧链异构.</p>
<p>不足: 现有方法不能应用到当前各种蛋白设计挑战上, 且未经过广泛实验验证.</p>
</dd>
</dl>
</section>
<section id="id3">
<h2>动机与尝试<a class="headerlink" href="#id3" title="Permalink to this heading"></a></h2>
<p>出发点: 一个有 3 个 encode &amp; 3 个decoder, hidden size = 128 的 MPNN <a class="reference internal" href="#ref1" id="id4"><span>[Ref1]</span></a>. 我
称为 baseMPNN.</p>
<dl>
<dt>baseMPNN</dt><dd><p>构成 encode/decoder 各 3 个. hiddne dimension = 128.</p>
<p>模型类型 autoregressive</p>
<p>序列预测顺序 从 N 端到 C 端.</p>
<dl class="simple">
<dt>输入</dt><dd><ul class="simple">
<li><p>Ca-Ca 距离</p></li>
<li><p>relative Ca-Ca frame orientations and rotations</p></li>
<li><p>backbone dihedral angles</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
<p>他们首先想要提升 baseMPNN 复原天然单链蛋白氨基酸序列的能力. 相关实验结果见
<a class="reference internal" href="#id12"><span class="std std-ref">论文结论</span></a>. 这里描述他们所做的试验. 首先准备数据集: 19,700 个来自 PDB 的高精度
单链结构, 用 CATH 数据库按 8:1:1 分成训练:测试:验证集.</p>
<p>试验 1: 发现若加入 N, CA, C, O 原子的坐标外加一个由其他骨架原子坐标计算出的虚拟 Cb 之间的
相互距离可以提升模型表现. 参见 <a class="reference internal" href="#id12"><span class="std std-ref">论文结论</span></a> 的 experiment 1.</p>
<p>试验 2: 在 backbone encoder 已有的 node update 机制的基础上添加 edge update 可
以提升性能. 参见 <a class="reference internal" href="#id12"><span class="std std-ref">论文结论</span></a> 的 experiment 2.</p>
<p>结合 试验 1 和试验 2 也可提升效果.  <a class="footnote-reference brackets" href="#f1" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>. 这也可以提升一点性能.</p>
<p>他们还研究了 K-nearest-Ca neighbor 神经网络如何影响氨基酸身份 [fig 发现效果在
32-48 时饱和. 作者的解释是: 因为氨基酸的 optimality 很大程度上依赖附近的蛋白环
境, 因此结构到序列的映射可以被 locally connected graph neural network 准确捕捉.</p>
</section>
<section id="id6">
<h2>ProtMPNN 设计的思路.<a class="headerlink" href="#id6" title="Permalink to this heading"></a></h2>
<p>为了能够应用到各种单链和多链序列设计任务上, 他们首先采用了顺序无关的decoding 顺
序: 在所有可能出现突变的位点随机采样. 这样可以用在类似 binder design 的任务上;
在此任务中, target 是固定的, 不会对 target 进行采样, 但是会将 target 包含到
sequence context 中.</p>
<p>设计 multichain protein 的策略:以 homodimer 为例, 分别预测两条链的 unnormalized
probabilities, 然后取平均,再获得 normalized probabilities.</p>
<p>multistate protein 的设计针对蛋白的每个态预测一个 unnormalized probability, 然
后将每个unnormalized probability 线性组合. 线性组合系数可是正数或负数, 标示着
UpWeight 或 DownWeight.</p>
<p>ProtMPNN 就是结合了上面两种策略的 MPNN 模型.</p>
<p>此外, 作者还发现在加入了标准差为 0.2A 的高斯噪声的蛋白质骨架上训练 ProtMPNN 可
以在 UniRef50 中的AF2结构上 (平均 pLDDT &gt; 80.0) 取得更好的 sequence recovery 效
果. 但是在有噪骨架上训练的模型在实验结构上表现不如在无噪声骨架上训练的
ProtMPNN.</p>
<section id="id7">
<h3>训练<a class="headerlink" href="#id7" title="Permalink to this heading"></a></h3>
<section id="single-chain-models">
<h4>训练 Single chain models<a class="headerlink" href="#single-chain-models" title="Permalink to this heading"></a></h4>
<dl>
<dt>基础信息</dt><dd><p>训练集: 19.7k 个来自 PDB 的高分辨率单链结构, 按 8:1:1 分成训练/测试/验证集.</p>
<p>学习率计划与初始化: 按原始 Transformer 论文 <a class="reference internal" href="#ref2" id="id8"><span>[Ref2]</span></a></p>
<p>dropout: 10%.</p>
<p>label smoothing rate: 10%.</p>
<p>batch size: 6000 tokens.</p>
<p>graph sparsity: 30 nearest neighbors using Ca-Ca distances.</p>
</dd>
</dl>
<p>对 baseMPNN 的调整:</p>
<ol class="arabic">
<li><p>边特征计算. 来自残基 <span class="math notranslate nohighlight">\(i, j\)</span> 的 N, Ca, C, O, Cb 坐标的相互距离
(5x5=25), 通过 16 个中心从 0 到 20A 均匀增长的高斯 RBF 函数, 最终每对残
基都会有 25x16=400个 <em>边特征</em>.</p></li>
<li><p>Edge update 的机制. 设 <span class="math notranslate nohighlight">\(V_i, V_j, E_{ij}\)</span> 是残基 <span class="math notranslate nohighlight">\(i, j\)</span> 的节
点 (vortex) 和边 (edge) 特征.</p>
<p>过程: 首先将 <span class="math notranslate nohighlight">\(V_i, V_j, E_{ij}\)</span> 连接成一个大张量, 通过一个 MLP 构造
<code class="docutils literal notranslate"><span class="pre">message</span></code> <span class="math notranslate nohighlight">\(M_{ij}\)</span>. <code class="docutils literal notranslate"><span class="pre">message</span></code> 沿着邻居维度 (j) 求和, 然后再用一
个 MLP 获得更新后的节点 <span class="math notranslate nohighlight">\(V_i^{new}\)</span>. 然后用这些新节点和一个 MLP 生
成新边, i.e. <span class="math notranslate nohighlight">\(E_{ij}^{new} = MLP(V_i^{new}, V_j^{new}, E_{ij})\)</span>. 这
里一共有 3 个 MLP [参见 <a class="reference internal" href="#model-arch"><span class="std std-ref">模型架构</span></a>]</p>
<p>所有的 MLP 层都应用了 Layer Normalization, dropout, and residual
connections. i.e.</p>
<div class="math notranslate nohighlight">
\[h_{new} = \mathrm{LN}\left[ h_{old} + Dropout(h) \right].\]</div>
</li>
<li><p>Random Decoder Order. 作者是通过即时构造置换矩阵来实现的.</p></li>
</ol>
</section>
<section id="multi-chain-models">
<h4>训练 multi chain models.<a class="headerlink" href="#multi-chain-models" title="Permalink to this heading"></a></h4>
<dl>
<dt>训练集:</dt><dd><p>来自 PDB 的 assembly (截至 2021-08-02); 测定方法为 X-ray crystallography 和
Cryo-EM; 分辨率优于 3.5A, 长度小于 1e4 个残基.</p>
<p>按 30% 序列相似度聚类, 工具为 MMseqs2. 最终获得 25361 个类.</p>
</dd>
</dl>
<p>训练时, 遍历所有 clusters, 从 cluster 中随机选一个序列 (query chain), 然后再随
机选一个构象(如果有多个构象). 最后重构 biological assembly. 若有多个 biological
assemblies, 则随机选一个.</p>
<p>对 hetero-oligomeric assemblies, mask 掉 query chain, 但将 biological assembly
中所有其他链的信息都告诉网络.</p>
<p>对 homo-oligomeric assemblies, mask 掉所有的链以避免信息泄露. 同源序列的认定标
准: TM-align 给出的序列相似性超过 70%.</p>
</section>
<section id="loss-functions-optimization">
<h4>Loss functions &amp; optimization<a class="headerlink" href="#loss-functions-optimization" title="Permalink to this heading"></a></h4>
<p>采用 log likelihood 作为 loss function, 10% label smoothing rate (不用标签
平滑结果也很好). 训练 loss 定义为 <span class="math notranslate nohighlight">\(sum(loss * mask) / 2000\)</span>, 2000 是
一个经验参数 <a class="footnote-reference brackets" href="#fn2" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>. <strong>这里的 mask 是什么?</strong></p>
</section>
</section>
<section id="input-features">
<span id="id10"></span><h3>Input features<a class="headerlink" href="#input-features" title="Permalink to this heading"></a></h3>
<p>ProtMPNN 不使用结点特征 (使用残基二面角作结点特征并未获得更好的结果, 因此简
洁起见不用任何结点特征).</p>
<dl class="field-list">
<dt class="field-odd">边特征<span class="colon">:</span></dt>
<dd class="field-odd"><p>包含残基间距离和序列内距离 (两个残基之间相距多少个残基). 序列内距离
用 relative positioning encoding 编码; 若残基在不同链上, 会有一指示
器.multichain model 的 RBF 中心是 2-22A 均匀分布的 16 个点.</p>
</dd>
<dt class="field-even">relative positioning encoding<span class="colon">:</span></dt>
<dd class="field-even"><p>采用类似 AlphaFold-2 的 one-hot 编码, 距离上
上限为 32, 即:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span><span class="mi">32</span><span class="p">,</span> <span class="o">-</span><span class="mi">31</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mf">32.</span>
</pre></div>
</div>
<p>若残基在其他链上, 还会有一个 33 的 token.</p>
<p>没有 positional encoding 也没有影响: “Ablating positional encodings
showed almost the same performance suggesting that relative primary
sequence or inter-chain information is already present in the
Euclidean distances between atoms, e.g. distances between
neighboring Cα atoms are the same for all residues.” [补充材料 p4]</p>
</dd>
</dl>
</section>
<section id="model-arch">
<span id="id11"></span><h3>模型架构<a class="headerlink" href="#model-arch" title="Permalink to this heading"></a></h3>
<p>使用 encoder-decoder message passing neural networks. [补充材料参考文献 1, 22,
28]. Encoder 用三个 hidden size = 128 的 MLP 更新结点和边特征 (好像没有结点特征,
参见 <a class="reference internal" href="#input-features"><span class="std std-ref">Input features</span></a>). 不用更大的 hidden size 是因为在验证集上这样做的收
益不大 (而 MLP 是很耗内存的). Decoder 将 encoder 的输出和蛋白质序列的 embedding
作为输入; Decoder layer 是 hidden size = 128 的有三个 layer 的原始 MPNN
layer.</p>
<p>Encoder 和 Decoder 的 MLP 都有三层线性层; 激活函数均为 GELU; Feedfoward 层都是
带 GELU 的两层线性层, 且中间 hidden size 比原始 Transformer 论文的大 4 倍.</p>
<p>Encoder Layer 的 forward 方法:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">encoder_layer_forward</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">E</span><span class="p">):</span>
    <span class="n">M_ij</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">[</span><span class="n">V_i</span><span class="p">,</span> <span class="n">V_j</span><span class="p">,</span> <span class="n">E_ij</span><span class="p">]</span>
    <span class="n">dV_i</span> <span class="o">=</span> <span class="n">Sum_j</span> <span class="p">[</span><span class="n">M_ij</span><span class="p">]</span>
    <span class="n">V_i</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">[</span><span class="n">V_i</span> <span class="o">+</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">dV_i</span><span class="p">)]</span>
    <span class="n">dV_i</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">[</span><span class="n">V_i</span><span class="p">]</span>
    <span class="n">V_i</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">[</span><span class="n">V_i</span> <span class="o">+</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">dV_i</span><span class="p">)]</span>
    <span class="n">dE_ij</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">[</span><span class="n">V_i</span><span class="p">,</span> <span class="n">V_j</span><span class="p">,</span> <span class="n">E_ij</span><span class="p">]</span>
    <span class="n">E_ij</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">[</span><span class="n">E_ij</span> <span class="o">+</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">dE_ij</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">V</span><span class="p">,</span> <span class="n">E</span>
</pre></div>
</div>
<p>Decoder Layer 的 forward 方法:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">decoder_layer_forward</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">E</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
    <span class="n">E_ij</span> <span class="o">=</span> <span class="n">Concat</span><span class="p">[</span><span class="n">E_ij</span><span class="p">,</span> <span class="n">S_j</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask_ij</span> <span class="o">+</span> <span class="n">Concat</span><span class="p">[</span><span class="n">E_ij</span><span class="p">,</span> <span class="mf">0.0</span><span class="o">*</span><span class="n">S_j</span><span class="p">]</span> <span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">mask_ij</span><span class="p">)</span>
    <span class="n">M_ij</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">[</span><span class="n">V_i</span><span class="p">,</span> <span class="n">V_j</span><span class="p">,</span> <span class="n">E_ij</span><span class="p">]</span>
    <span class="n">dV_i</span> <span class="o">=</span> <span class="n">Sum_j</span> <span class="p">[</span><span class="n">M_ij</span><span class="p">]</span>
    <span class="n">V_i</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">[</span><span class="n">V_i</span> <span class="o">+</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">dV_i</span><span class="p">)]</span>
    <span class="n">dV_i</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">[</span><span class="n">V_i</span><span class="p">]</span>
    <span class="n">V_i</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">[</span><span class="n">V_i</span> <span class="o">+</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">dV_i</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">V</span>
</pre></div>
</div>
<p><strong>问题</strong> <code class="docutils literal notranslate"><span class="pre">mask</span></code> 是什么? 前面有印象说是上三角矩阵, 但是其元素值怎么确定呢?</p>
</section>
<section id="in-sillico-analysis">
<h3>更多虚拟验证 (in sillico analysis)<a class="headerlink" href="#in-sillico-analysis" title="Permalink to this heading"></a></h3>
<dl>
<dt>和 Rosetta fixed backbone design 比较</dt><dd><p>ProtMPNN 在 402 monomer 测试集上, 显著更优 (recovery 52.4% v.s. 32.9%). 此
外还观察到两种模型的序列 recoveries 存在相关性 (fig.S12).</p>
</dd>
<dt>氨基酸偏好</dt><dd><p>Fig.S2 描述了 ProtMPNN 和 Rosetta 的氨基酸偏好.</p>
<p>Rosetta: overrepresentation of alanines in the core and boundary.</p>
<p>ProtMPNN: its amino acid bias is a function of the sampling temperature
(Fig.S6); low temperatures introducing more charged amino acids on the
surface (这说明原始的 logits 偏好带电氨基酸).</p>
<p>Both: favour a negatively charged glutamic acid, E, on the surface; and
disfavour a polar amino acid glutamine, Q.</p>
<p>ProtMPNN and Rosetta biases strongly disagree for lysine, K, on the
surface.</p>
</dd>
</dl>
</section>
<section id="id12">
<span id="id13"></span><h3>论文结论<a class="headerlink" href="#id12" title="Permalink to this heading"></a></h3>
<img alt="_images/table1.png" src="_images/table1.png" />
</section>
<section id="id14">
<h3>我的问题<a class="headerlink" href="#id14" title="Permalink to this heading"></a></h3>
<p>CATH</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="f1" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">1</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://www.science.org/doi/10.1126/science.add2187#sec-1">https://www.science.org/doi/10.1126/science.add2187#sec-1</a></p>
</aside>
<aside class="footnote brackets" id="fn2" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">2</a><span class="fn-bracket">]</span></span>
<p>我猜测是想把 loss 缩放到一个合理的大小, 避免过大的 loss.</p>
</aside>
</aside>
<div role="list" class="citation-list">
<div class="citation" id="ref1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">Ref1</a><span class="fn-bracket">]</span></span>
<p>J. Ingraham, V. K. Garg, R. Barzilay, T. Jaakkola, “Generative
models for graph-based protein design” in Advances in Neural
Information Processing Systems 32 (NeurIPS
2019), H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. Fox, R. Garnett,
Eds. (Neural Information Processing Systems Foundation, 2019),
pp. 15741–15752.</p>
</div>
<div class="citation" id="ref2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">Ref2</a><span class="fn-bracket">]</span></span>
<p>A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin,
“Attention is all you need” in Advances in Neural Information
Processing Systems 30 (NeurIPS 2017), I. Guyon, U. Von
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, R. Garnett,
Eds. (Neural Information Processing Systems Foundation, 2017),
pp. 5999–6009.</p>
</div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="daily/git_usage.html" class="btn btn-neutral float-left" title="git 用法教程" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="glossary_ai.html" class="btn btn-neutral float-right" title="人工智能方面的术语" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, jmlv.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>